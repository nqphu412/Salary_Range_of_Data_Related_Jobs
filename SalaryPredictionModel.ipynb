{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "054b9e1e-d395-4521-9378-4258fbe76e8d",
   "metadata": {},
   "source": [
    "# Introduction\n",
    "This project analyses a data-jobs dataset from Kaggle to uncover salary trends, in-demand skills, and role expectations across the data industry around the world. It also features a predictive model that allows users to “name their personal price” by entering their target role, skills, company location, and experience to estimate a realistic market-aligned salary range for better career planning and negotiation."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9d3ea831-297c-4a7f-a13e-958d028512cf",
   "metadata": {},
   "source": [
    "# Import data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "0156d06c-307d-4597-aebf-ed31772e5e49",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn.preprocessing import MultiLabelBinarizer\n",
    "from sklearn.model_selection import train_test_split, RandomizedSearchCV\n",
    "from sklearn.metrics import mean_squared_error, r2_score\n",
    "\n",
    "import requests\n",
    "\n",
    "import xgboost as xgb\n",
    "from xgboost import XGBRegressor # Main model for this prediction\n",
    "\n",
    "import onnxmltools\n",
    "from onnxmltools.convert.common.data_types import FloatTensorType # Save model for site\n",
    "\n",
    "import json # Save columns names for onnx"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "880d19aa-7d86-446c-99b4-fa92bbe8de4a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'pandas.core.frame.DataFrame'>\n",
      "RangeIndex: 944 entries, 0 to 943\n",
      "Data columns (total 13 columns):\n",
      " #   Column           Non-Null Count  Dtype \n",
      "---  ------           --------------  ----- \n",
      " 0   job_title        941 non-null    object\n",
      " 1   seniority_level  884 non-null    object\n",
      " 2   status           688 non-null    object\n",
      " 3   company          944 non-null    object\n",
      " 4   location         942 non-null    object\n",
      " 5   post_date        944 non-null    object\n",
      " 6   headquarter      944 non-null    object\n",
      " 7   industry         944 non-null    object\n",
      " 8   ownership        897 non-null    object\n",
      " 9   company_size     944 non-null    object\n",
      " 10  revenue          929 non-null    object\n",
      " 11  salary           944 non-null    object\n",
      " 12  skills           944 non-null    object\n",
      "dtypes: object(13)\n",
      "memory usage: 96.0+ KB\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>job_title</th>\n",
       "      <th>seniority_level</th>\n",
       "      <th>status</th>\n",
       "      <th>company</th>\n",
       "      <th>location</th>\n",
       "      <th>post_date</th>\n",
       "      <th>headquarter</th>\n",
       "      <th>industry</th>\n",
       "      <th>ownership</th>\n",
       "      <th>company_size</th>\n",
       "      <th>revenue</th>\n",
       "      <th>salary</th>\n",
       "      <th>skills</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>data scientist</td>\n",
       "      <td>senior</td>\n",
       "      <td>hybrid</td>\n",
       "      <td>company_003</td>\n",
       "      <td>Grapevine, TX . Hybrid</td>\n",
       "      <td>17 days ago</td>\n",
       "      <td>Bentonville, AR, US</td>\n",
       "      <td>Retail</td>\n",
       "      <td>Public</td>\n",
       "      <td>€352.44B</td>\n",
       "      <td>Public</td>\n",
       "      <td>€100,472 - €200,938</td>\n",
       "      <td>['spark', 'r', 'python', 'scala', 'machine lea...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>data scientist</td>\n",
       "      <td>lead</td>\n",
       "      <td>hybrid</td>\n",
       "      <td>company_005</td>\n",
       "      <td>Fort Worth, TX . Hybrid</td>\n",
       "      <td>15 days ago</td>\n",
       "      <td>Detroit, MI, US</td>\n",
       "      <td>Manufacturing</td>\n",
       "      <td>Public</td>\n",
       "      <td>155,030</td>\n",
       "      <td>€51.10B</td>\n",
       "      <td>€118,733</td>\n",
       "      <td>['spark', 'r', 'python', 'sql', 'machine learn...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "        job_title seniority_level  status      company  \\\n",
       "0  data scientist          senior  hybrid  company_003   \n",
       "1  data scientist            lead  hybrid  company_005   \n",
       "\n",
       "                  location    post_date          headquarter       industry  \\\n",
       "0   Grapevine, TX . Hybrid  17 days ago  Bentonville, AR, US         Retail   \n",
       "1  Fort Worth, TX . Hybrid  15 days ago      Detroit, MI, US  Manufacturing   \n",
       "\n",
       "  ownership company_size  revenue               salary  \\\n",
       "0    Public     €352.44B   Public  €100,472 - €200,938   \n",
       "1    Public      155,030  €51.10B             €118,733   \n",
       "\n",
       "                                              skills  \n",
       "0  ['spark', 'r', 'python', 'scala', 'machine lea...  \n",
       "1  ['spark', 'r', 'python', 'sql', 'machine learn...  "
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "full_df = pd.read_csv('data_science_job_posts_2025.csv')\n",
    "full_df.info()\n",
    "full_df.head(2)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0b036dc3-3ed3-4587-857f-c6b40c19aa3e",
   "metadata": {},
   "source": [
    "# Cleaning\n",
    "Some key factors that are important to spot out the range include job title, seniority level, location (country-based or continent-based for generalised data), industry, ownership, company size and skill set of candidates.\n",
    "- Omit observations with null value for job title, which is one of the crucial factors (only 3 observations removed)\n",
    "- Assign mid level for seniority level for null spots\n",
    "- Using headquarter to extract the country based and map them to continent data\n",
    "- Salary column will be divided into starting point and ending point and be converted into AUD\n",
    "- Skills will be separated into single column for each of them, can be combined later based on the type of skills\n",
    "- Others will be deleted"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c6fdb03d-0529-4674-b28a-746ebdbb7655",
   "metadata": {},
   "source": [
    "### Remove some unwanted columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "7332b3e4-3b4c-44ba-94c8-f6a059a8d47d",
   "metadata": {},
   "outputs": [],
   "source": [
    "df = full_df.drop(columns=[\"company\", \"location\", \"post_date\", \"revenue\", \"company_size\"])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "72b85ac6-b08b-470b-bea0-cb0fccf021fe",
   "metadata": {},
   "source": [
    "### Dealing with null value in job title and seniority level\n",
    "In terms of job title, due to the significant gap between among titles, this model mainly will focus on position data scientist and machine learning engineer. Although the number of machine learning engineer observations is overshadowed, there are still chances to get it train properly while doing random splitting. The result might be skewed biased towards data scientist observations.\n",
    "\n",
    "Moreover, regarding to seniority level, there are also gaps among levels. As a result, all null value will be assigned with the junior level."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "b3fe21f0-fc57-4191-b900-82ef181689f7",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "job_title\n",
       "data scientist               856\n",
       "machine learning engineer     80\n",
       "data engineer                  4\n",
       "NaN                            3\n",
       "data analyst                   1\n",
       "Name: count, dtype: int64"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df['job_title'].value_counts(dropna=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "d89dbf44-cd4d-4925-83dc-992544d10aca",
   "metadata": {},
   "outputs": [],
   "source": [
    "df = df[df['job_title'].isin(['data scientist', 'machine learning engineer'])]\n",
    "df = df.dropna(subset = ['job_title'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "781cbcf4-ff70-4c7d-902e-06f7db7dd870",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "seniority_level\n",
       "senior      626\n",
       "lead        115\n",
       "midlevel    110\n",
       "NaN          60\n",
       "junior       25\n",
       "Name: count, dtype: int64"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df['seniority_level'].value_counts(dropna=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "dd6412e0-1cb0-444c-8d71-7b9ec4110720",
   "metadata": {},
   "outputs": [],
   "source": [
    "seniority_map = {\n",
    "    'junior': 1,\n",
    "    'midlevel': 2,\n",
    "    'senior': 3,\n",
    "    'lead': 4\n",
    "}\n",
    "\n",
    "df['seniority_level'] = df['seniority_level'].map(seniority_map)\n",
    "df['seniority_level'] = df['seniority_level']"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ee7b9ede-8c33-476d-9898-10f74ec3d6fa",
   "metadata": {},
   "source": [
    "### Mapping to continent"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "3b05a698-e305-4bae-b983-f90c733d39b3",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "country\n",
      "US    765\n",
      "IN     27\n",
      "IE     22\n",
      "GB     22\n",
      "CA     22\n",
      "FR     14\n",
      "DE     11\n",
      "SG     10\n",
      "CH      7\n",
      "NL      7\n",
      "EE      6\n",
      "CN      4\n",
      "DK      4\n",
      "TW      3\n",
      "SE      3\n",
      "AU      3\n",
      "IT      2\n",
      "BR      1\n",
      "AT      1\n",
      "JP      1\n",
      "ES      1\n",
      "Name: count, dtype: int64\n"
     ]
    }
   ],
   "source": [
    "df['country'] = df['headquarter'].str[-2:]\n",
    "print(df['country'].value_counts())"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f065e51c-afca-426b-ae5a-450ea8956142",
   "metadata": {},
   "source": [
    "There are a lot of observation with the quantity less than 10. Hence, because those locations exist but dont have any clear reliable pattern, I'll treat them as general non-major markets, which will be considered **'Others'** in the continent column.\n",
    "\n",
    "So now in the continent columns, we will only have North America, Europe, Asia, and Others.\n",
    "\n",
    "Moreover, due to the extreme value of the US market, another column named **isUS** will be added, which could balance the data and give some insights among other countries."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "c9578193-96ee-4104-92f9-8ccb1a6e9ac1",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Mapping to continent\n",
    "continent_map = {\n",
    "    'US': 'North America',\n",
    "    'CA': 'North America',\n",
    "    'GB': 'Europe',\n",
    "    'DE': 'Europe',\n",
    "    'FR': 'Europe',\n",
    "    'IE': 'Europe',\n",
    "    'IN': 'Asia',\n",
    "    'SG': 'Asia'\n",
    "}\n",
    "\n",
    "df['continent'] = df['country'].map(continent_map).fillna('Others')\n",
    "df = df.drop(columns = ['headquarter'])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2f48548c-3465-4ce5-93f4-aff87198f8a4",
   "metadata": {},
   "source": [
    "### Dividing salary to starting and ending points"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "6d0ed783-232e-44ab-a162-17883869426e",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_exchange_rate(from_currency='EUR', to_currency='AUD'):\n",
    "    url = f\"https://api.frankfurter.app/latest?from={from_currency}&to={to_currency}\"\n",
    "    response = requests.get(url)\n",
    "    data = response.json()\n",
    "    return data['rates'][to_currency]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "75b0ab28-2da1-47fe-9f42-b639cceaa898",
   "metadata": {},
   "outputs": [],
   "source": [
    "def addSalaryRange(colRange, exchange_rate = get_exchange_rate()):\n",
    "    # Remove currency and commas\n",
    "    cleanRange = colRange.replace('€', '').replace(',', '').strip()\n",
    "\n",
    "    # Split column and convert to AUD\n",
    "    if '-' in cleanRange:\n",
    "        start, end = cleanRange.split('-')\n",
    "        start = float(start.strip()) * exchange_rate\n",
    "        end   = float(end.strip()) * exchange_rate\n",
    "    else: \n",
    "        start = end = float(cleanRange) * exchange_rate\n",
    "\n",
    "    return pd.Series([start, end])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "22314b65-4f95-4d5f-8def-56bee37c02a4",
   "metadata": {},
   "outputs": [],
   "source": [
    "df[['salary_start_aud', 'salary_end_aud']] = df['salary'].apply(addSalaryRange)\n",
    "df['range_log'] = np.log1p(df['salary_end_aud'] - df['salary_start_aud']) # Can give more information\n",
    "df = df.drop(columns=[\"salary\"])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c174e556-4d9b-4643-9e79-142ab099b71d",
   "metadata": {},
   "source": [
    "### Separating skills"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "fc280b9f-e085-47cb-982c-7b3092f8e4cb",
   "metadata": {},
   "outputs": [],
   "source": [
    "def cleanSkills(skills): # Need to transform it to list type, to work with MultiLabelBinarizer\n",
    "    cleanSkills = skills.replace('[','').replace(']','').replace(\"'\",'')\n",
    "    if cleanSkills == '':\n",
    "        return ['None'] #### Later just need to remove the column named 'None'\n",
    "    cleanSkills = cleanSkills.split(', ')\n",
    "    return cleanSkills\n",
    "\n",
    "df['skills'] = df['skills'].apply(cleanSkills)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "93dd70bc-26ed-4021-a944-617f5ddb798d",
   "metadata": {},
   "outputs": [],
   "source": [
    "skills_list = []\n",
    "\n",
    "for lists in df['skills']:\n",
    "    for listX in lists:\n",
    "        if listX not in skills_list: skills_list.append(listX)\n",
    "\n",
    "mlb = MultiLabelBinarizer(classes = skills_list)\n",
    "skills_df = pd.DataFrame(mlb.fit_transform(df['skills']), columns = skills_list)\n",
    "\n",
    "## Remove None\n",
    "skills_df = skills_df.drop(columns = ['None'])\n",
    "skills_list.remove('None')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8f525a52-e94e-4f49-b4b3-b5fa56fbe6a8",
   "metadata": {},
   "source": [
    "### Creating dummy instead of categorical data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "254ea6a8-e18a-418d-ac68-eb64015d8ca7",
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.get_dummies(df,\n",
    "                    columns = ['job_title', 'status', 'industry', 'ownership', 'continent', 'country'],\n",
    "                    dummy_na = False, ## XGBoost can work with null value\n",
    "                    drop_first = False, \n",
    "                    dtype = int).drop(columns = ['skills'])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "acfc53ab-fbf1-4a0e-aa38-2ac93966a211",
   "metadata": {},
   "source": [
    "### Concatenate everything together"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "1b6caa3d-c7a2-4cbf-beda-1108ff437be4",
   "metadata": {},
   "outputs": [],
   "source": [
    "detailed_df = pd.concat([df.reset_index(drop=True), skills_df.reset_index(drop=True)], axis=1)\n",
    "\n",
    "## Only keep 90% of dataset\n",
    "detailed_df = detailed_df[\n",
    "    (detailed_df['salary_start_aud'] >= detailed_df['salary_start_aud'].quantile(0.01)) &\n",
    "    (detailed_df['salary_start_aud'] <= detailed_df['salary_start_aud'].quantile(0.99))\n",
    "]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e2fa177a-22aa-4293-b049-d121a8e027f5",
   "metadata": {},
   "source": [
    "# Model build"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "da756a3d-e84a-449f-8169-21903129cfa9",
   "metadata": {},
   "source": [
    "## Using table with detailed skills "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "ee7063af-aaf0-423e-b96f-3546aed593ef",
   "metadata": {},
   "outputs": [],
   "source": [
    "X = detailed_df.drop(columns=[\"salary_start_aud\", \"salary_end_aud\"]).reset_index(drop=True)\n",
    "y = detailed_df[[\"salary_start_aud\", \"salary_end_aud\"]].reset_index(drop=True)\n",
    "y_log = np.log1p(y) # Mainly used due to big variance when working with money data\n",
    "\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y_log,\n",
    "                                                    test_size = 0.2,\n",
    "                                                    random_state = 412,\n",
    "                                                    shuffle = True)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e117e81c-c659-40ab-824d-f983cbbc806a",
   "metadata": {},
   "source": [
    "#### Starting point"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "8e183efd-c211-41ce-86ac-6bcc1c29db2c",
   "metadata": {},
   "outputs": [],
   "source": [
    "xgb_start = XGBRegressor()\n",
    "\n",
    "param_grid = {\n",
    "    'n_estimators': [500, 700, 1000],\n",
    "    'max_depth': [5, 6, 7, 8, 9],\n",
    "    'learning_rate': [0.01, 0.05, 0.1], \n",
    "    'reg_alpha': [0, 0.01, 0.05, 0.1],\n",
    "    'reg_lambda': [1, 1.5, 2],\n",
    "    'subsample': [0.6, 0.8, 1.0],\n",
    "    'colsample_bytree': [0.6, 0.8, 1.0],\n",
    "    'gamma': [0, 0.1, 0.2, 0.5]\n",
    "}\n",
    "\n",
    "random_search_start = RandomizedSearchCV(estimator = xgb_start,\n",
    "                                         param_distributions = param_grid,\n",
    "                                         n_iter = 50, scoring = 'r2', cv = 5)\n",
    "\n",
    "random_search_start.fit(X_train, y_train[\"salary_start_aud\"])\n",
    "\n",
    "model_start = random_search_start.best_estimator_\n",
    "model_start.fit(X_train, y_train[\"salary_start_aud\"])\n",
    "\n",
    "y_pred_start = model_start.predict(X_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "f5f4bfa8-e5ad-4b51-ae77-12987a9d05d1",
   "metadata": {},
   "outputs": [],
   "source": [
    "tab_start = pd.DataFrame({'actual_salary': np.expm1(y_test[\"salary_start_aud\"]),\n",
    "                          'prediction_salary': np.expm1(y_pred_start),\n",
    "                          'error_salary': np.expm1(y_test[\"salary_start_aud\"]) - np.expm1(y_pred_start),\n",
    "                          'abs%_error': abs(np.expm1(y_test[\"salary_start_aud\"]) - np.expm1(y_pred_start)) / np.expm1(y_test[\"salary_start_aud\"]),\n",
    "                          'actual_log': y_test[\"salary_start_aud\"],\n",
    "                          'prediction_log': y_pred_start})"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "09ee9154-3a1f-46fb-9791-df9d188aef20",
   "metadata": {},
   "source": [
    "#### Ending point"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "982ea029-da46-4bdb-b62c-3881fcdd10e3",
   "metadata": {},
   "outputs": [],
   "source": [
    "xgb_end = XGBRegressor()\n",
    "\n",
    "param_grid = { # Expect to gain deeper pattern compared to starting range\n",
    "    'n_estimators': [1000, 1200, 1500],\n",
    "    'max_depth': [7, 8, 9, 10, 12],\n",
    "    'learning_rate': [0.01, 0.02, 0.05], \n",
    "    'reg_alpha': [0, 0.01, 0.05, 0.1],\n",
    "    'reg_lambda': [1, 1.5, 2],\n",
    "    'subsample': [0.6, 0.8, 1.0],\n",
    "    'colsample_bytree': [0.6, 0.8, 1.0],\n",
    "    'gamma': [0.2, 0.5, 1]\n",
    "}\n",
    "\n",
    "random_search_end = RandomizedSearchCV(estimator = xgb_end, \n",
    "                                       param_distributions = param_grid,\n",
    "                                       n_iter = 50, scoring = 'r2', cv = 5)\n",
    "\n",
    "random_search_end.fit(X_train, y_train[\"salary_end_aud\"])\n",
    "\n",
    "model_end = random_search_end.best_estimator_\n",
    "model_end.fit(X_train, y_train[\"salary_end_aud\"])\n",
    "\n",
    "y_pred_end = model_end.predict(X_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "738277be-363b-4a5d-920d-a543486d9c32",
   "metadata": {},
   "outputs": [],
   "source": [
    "tab_end = pd.DataFrame({'actual_salary': np.expm1(y_test[\"salary_end_aud\"]),\n",
    "                        'prediction_salary': np.expm1(y_pred_end),\n",
    "                        'error_salary': np.expm1(y_test[\"salary_end_aud\"]) - np.expm1(y_pred_end),\n",
    "                        'abs%_error': abs(np.expm1(y_test[\"salary_end_aud\"]) - np.expm1(y_pred_end)) / np.expm1(y_test[\"salary_end_aud\"]),\n",
    "                        'actual_log': y_test[\"salary_end_aud\"],\n",
    "                        'prediction_log': y_pred_end})"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2bf291c1-a009-40f6-914e-bf2a52ad0f4e",
   "metadata": {},
   "source": [
    "### Analysis"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9f16a226-8a79-4ce9-9d39-418329a20ced",
   "metadata": {},
   "source": [
    "#### Model assessment"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0b64b659-a8d0-4fa8-85c8-eaf7ae24b241",
   "metadata": {},
   "outputs": [],
   "source": [
    "r2_start = r2_score(tab_start['actual_log'], tab_start['prediction_log'])\n",
    "r2_end   = r2_score(tab_end['actual_log'], tab_end['prediction_log'])\n",
    "\n",
    "print('Model explains', round(r2_start * 100, 2), 'percent of the variation in starting salary')\n",
    "print('Model explains', round(r2_end * 100, 2), 'percent of the variation in ending salary')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "840212df-68fc-4807-b3d0-a07f93b5b5ec",
   "metadata": {},
   "source": [
    "With **majority of the variation in salary ranges explained**, the model demonstrates moderate predictive performance. This suggests that key factors such as job role, experience, location, and skill sets are meaningful drivers in capturing salary patterns within the data. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "579ff9d4-89d0-457f-aa03-95007818d8a8",
   "metadata": {},
   "outputs": [],
   "source": [
    "fig, axes = plt.subplots(ncols=2, figsize=(14, 6))\n",
    "\n",
    "# Distribution of Actual Starting Range Data\n",
    "axes[0].hist(np.expm1(y_test[\"salary_start_aud\"]), edgecolor='black')\n",
    "axes[0].set_title(\"Distribution of Actual Starting Range Data\")\n",
    "axes[0].set_xlabel(\"Starting Point\")\n",
    "axes[0].set_ylabel(\"Frequency\")\n",
    "\n",
    "# Boxplot of Actual vs Predicted Salary Distributions\n",
    "axes[1].boxplot([tab_start['actual_salary'], tab_start['prediction_salary'], \n",
    "                 tab_end['actual_salary'], tab_end['prediction_salary']],\n",
    "                 tick_labels=['Actual Start', 'Pred Start', 'Actual End', 'Pred End'])\n",
    "axes[1].set_title(\"Actual vs Predicted Salary Distributions\")\n",
    "axes[1].set_ylabel(\"Salary\")\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c039d772-b3fc-40b5-96d1-6bce173fa2cb",
   "metadata": {},
   "source": [
    "The boxplot indicates that the model successfully captures the bulk of the salary distribution, as **most predicted values fall within the interquartile range** of the actual data. This suggests that the model generalises well for typical cases.\n",
    "\n",
    "However, the model struggles with higher salary ranges, where a few observations are under-predicted compared to the actual values. This is likely due to the imbalance in the original dataset — only two job postings have starting salaries around $400,000 — so **the model lacks sufficient examples to confidently predict extreme values**."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fb1a585f-2314-4975-ac94-6ad4eac51582",
   "metadata": {},
   "outputs": [],
   "source": [
    "tab_start['error_band'] = pd.cut(\n",
    "    tab_start['abs%_error'],\n",
    "    bins=[0, 0.3, 1],\n",
    "    labels=['Less than 30%', 'More than 30%']\n",
    ")\n",
    "\n",
    "tab_end['error_band'] = pd.cut(\n",
    "    tab_end['abs%_error'],\n",
    "    bins=[0, 0.3, 1],\n",
    "    labels=['Less than 30%', 'More than 30%']\n",
    ")\n",
    "\n",
    "print('Starting salary prediction accuracy:')\n",
    "print(tab_start['error_band'].value_counts(normalize=True)*100)\n",
    "\n",
    "print('\\nEnding salary prediction accuracy:')\n",
    "print(tab_end['error_band'].value_counts(normalize=True)*100)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b09d579a-2458-46a9-9c10-53305e723627",
   "metadata": {},
   "source": [
    "**Most salary predictions fall within 30%** of the actual salary values, indicating that the model provides **reasonably reliable estimates** for the majority of job postings. This approach offers a more intuitive and practical assessment of performance, reflecting how the model may be **used in real-world salary benchmarking and career planning**."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cf683d6e-3794-41d7-9fea-729236381fbe",
   "metadata": {},
   "outputs": [],
   "source": [
    "fig, axes = plt.subplots(ncols=2, figsize=(14, 6))\n",
    "\n",
    "# Gain - start\n",
    "xgb.plot_importance(model_start, max_num_features = 5, importance_type = 'gain', ax = axes[0])\n",
    "axes[0].set_title(\"Top 5 Most Important Features (Gain) - Start\")\n",
    "\n",
    "# Gain - end\n",
    "xgb.plot_importance(model_end, max_num_features = 5, importance_type = 'gain', ax = axes[1])\n",
    "axes[1].set_title(\"Top 5 Most Important Features (Gain) - End\")\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0f545575-1ab3-4bd7-91a8-3e2552e4993a",
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.hist(detailed_df['salary_start_aud'], edgecolor='black')\n",
    "plt.xlabel(\"Starting Salary\")\n",
    "plt.ylabel(\"Frequency\")\n",
    "plt.title(\"Distribution of Actual Starting Range Data\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fc59adbc-ab49-424f-99a4-6791c0c5f227",
   "metadata": {},
   "outputs": [],
   "source": [
    "print('There are', sum(detailed_df['country_US'] == 1), 'posts from the US company in', len(detailed_df))\n",
    "\n",
    "fig, axes = plt.subplots(1, 2, figsize=(12, 6))\n",
    "\n",
    "us_mask = X_test['country_US'] == 1\n",
    "non_us_mask = X_test['country_US'] == 0\n",
    "\n",
    "# Starting range graph\n",
    "ax = axes[0]\n",
    "ax.scatter(tab_start.loc[us_mask, 'actual_salary'], \n",
    "           tab_start.loc[us_mask, 'prediction_salary'],\n",
    "           label = 'US')\n",
    "ax.scatter(tab_start.loc[non_us_mask, 'actual_salary'], \n",
    "           tab_start.loc[non_us_mask, 'prediction_salary'],\n",
    "           label = 'Non-US')\n",
    "ax.legend()\n",
    "\n",
    "# Line of perfect prediction\n",
    "min_val = min(np.min(tab_start['actual_salary']), np.min(tab_start['prediction_salary']))\n",
    "max_val = max(np.max(tab_start['actual_salary']), np.max(tab_start['prediction_salary']))\n",
    "ax.plot([min_val, max_val], [min_val, max_val], color = 'red', alpha = 0.4)\n",
    "\n",
    "ax.fill_between([min_val, max_val],\n",
    "    [min_val*0.7, max_val*0.7],\n",
    "    [min_val*1.3, max_val*1.3],\n",
    "    color = 'red', alpha = 0.1)\n",
    "\n",
    "ax.set_xlabel(\"Actual Salary\")\n",
    "ax.set_ylabel(\"Predicted Salary\")\n",
    "ax.set_title(\"Start Salary: Actual vs Predicted\")\n",
    "\n",
    "# Ending range graph\n",
    "ax = axes[1]\n",
    "ax.scatter(tab_end.loc[us_mask, 'actual_salary'], \n",
    "           tab_end.loc[us_mask, 'prediction_salary'],\n",
    "           label = 'US')\n",
    "ax.scatter(tab_end.loc[non_us_mask, 'actual_salary'], \n",
    "           tab_end.loc[non_us_mask, 'prediction_salary'],\n",
    "           label = 'Non-US')\n",
    "ax.legend()\n",
    "\n",
    "# Line of perfect prediction\n",
    "min_val = min(np.min(tab_end['actual_salary']), np.min(tab_end['prediction_salary']))\n",
    "max_val = max(np.max(tab_end['actual_salary']), np.max(tab_end['prediction_salary']))\n",
    "ax.plot([min_val, max_val], [min_val, max_val], color = 'red', alpha = 0.4)\n",
    "\n",
    "ax.fill_between([min_val, max_val],\n",
    "    [min_val*0.7, max_val*0.7],\n",
    "    [min_val*1.3, max_val*1.3],\n",
    "    color = 'red', alpha = 0.1)\n",
    "\n",
    "ax.set_xlabel(\"Actual Salary\")\n",
    "ax.set_ylabel(\"Predicted Salary\")\n",
    "ax.set_title(\"End Salary: Actual vs Predicted\")\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "76dd0fa6-a623-439b-92c4-e8fedb9c038e",
   "metadata": {},
   "source": [
    "Visually, the scatter plots illustrate how many observations fall within the 30% error band. Overall, more than 75% of the predictions lie within this region, indicating that both models demonstrate reasonably strong predictive performance. However, several notable errors remain and can be explained by characteristics of the dataset.\n",
    "\n",
    "Consistent with the boxplot results, only a small number of observations (out of more than 900 samples) represent very high salary ranges. This limited representation makes it challenging for the models to learn reliable patterns for these cases, leading to a tendency to underpredict high-salary positions.\n",
    "\n",
    "Conversely, a number of lower-salary observations are overpredicted. The dataset is heavily dominated by US job postings (749 out of 916), with salaries largely concentrated in the mid-to-high ranges. As a result, features such as **country_US** and **continent_North America** (in top 5 features with great importance to models) become strong predictive signals, and the models may bias predictions toward higher salary levels when these features are present.\n",
    "\n",
    "These limitations are primarily data-related rather than model-related. Performance could be further improved by collecting more balanced data across regions and salary levels, particularly for non-US markets and lower-paying roles. Nevertheless, the current models still provide sufficiently reliable estimates for practical use, such as helping job seekers approximate their expected salary range when “naming their price.”"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5f1d473c-d374-48d4-8843-0203ef471722",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_plot = pd.DataFrame({\n",
    "    'actual_start': tab_start['actual_salary'],\n",
    "    'actual_end': tab_end['actual_salary'],\n",
    "    'pred_start': tab_start['prediction_salary'],\n",
    "    'pred_end': tab_end['prediction_salary']\n",
    "})\n",
    "\n",
    "# Sorted by actual starting point (Clear visualisation for range)\n",
    "df_plot = df_plot.sort_values(by='actual_start').reset_index(drop=True)\n",
    "\n",
    "idx = range(len(df_plot))\n",
    "\n",
    "plt.figure(figsize=(14,6))\n",
    "\n",
    "# Actual salary range\n",
    "plt.fill_between(idx, df_plot['actual_start'], df_plot['actual_end'], \n",
    "                 color = 'orange', alpha = 0.4, label = 'Actual Range')\n",
    "\n",
    "# Predicted salary range\n",
    "plt.fill_between(idx, df_plot['pred_start'], df_plot['pred_end'], \n",
    "                 color = 'lightblue', alpha = 0.4, label = 'Predicted Range')\n",
    "\n",
    "plt.title(\"Actual vs Predicted Salary Ranges\")\n",
    "plt.ylabel(\"Salary\")\n",
    "plt.ylim(min(df_plot.min()), max(df_plot.max()))\n",
    "plt.xticks([])\n",
    "plt.legend()\n",
    "plt.grid(True, linestyle = '-', alpha = 0.3, lw = 0.5)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "018482f8-de26-4827-bc37-c40688b6e178",
   "metadata": {},
   "source": [
    "Finally, this graph shows how the predicted salaries compare to the actual ranges. Similar to the previous plots, the overlapping points are concentrated primarily in the mid-range, roughly between $\\textdollar$120,000 and $\\textdollar$300,000. Predictions in this range align closely with actual salaries, indicating that the model is most confident here. At the lower and higher extremes, predictions are more dispersed, reflecting the limited number of examples and higher uncertainty in these regions. Overall, this suggests the model performs reliably for typical mid-range salaries, which is enough for users to rely on as a foundation when trying to negotiate salary band in the future.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dfaf86bf-d23a-46e7-a6b1-1c7399c2d880",
   "metadata": {},
   "source": [
    "# Save model to create site for user-friendly interaction"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0337ee37-bd9b-4c7c-b70d-a172203285b9",
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train_np = X_train.values.astype(np.float32)\n",
    "\n",
    "# Retrain\n",
    "model_start = random_search_start.best_estimator_\n",
    "model_start.fit(X_train_np, y_train[\"salary_start_aud\"])\n",
    "\n",
    "model_end = random_search_end.best_estimator_\n",
    "model_end.fit(X_train_np, y_train[\"salary_end_aud\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "355f5a6c-bd23-40f8-b96e-cc38daafb6a1",
   "metadata": {},
   "outputs": [],
   "source": [
    "feature_order = list(X_train.columns)\n",
    "n_features = X_train_np.shape[1]\n",
    "initial_type = [('input', FloatTensorType([None, n_features]))]\n",
    "\n",
    "onnx_start = onnxmltools.convert_xgboost(model_start.get_booster(), initial_types=initial_type)\n",
    "onnx_end = onnxmltools.convert_xgboost(model_end.get_booster(), initial_types=initial_type)\n",
    "\n",
    "with open(\"salary_start_model.onnx\", \"wb\") as f:\n",
    "    f.write(onnx_start.SerializeToString())\n",
    "\n",
    "with open(\"salary_end_model.onnx\", \"wb\") as f:\n",
    "    f.write(onnx_end.SerializeToString())\n",
    "\n",
    "with open(\"feature_order.json\", \"w\") as f:\n",
    "    json.dump(feature_order, f)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
